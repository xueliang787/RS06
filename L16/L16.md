Thinking1 机器学习中的监督学习、非监督学习、强化学习有何区别

    强化学习与监督学习的区别：强化学习没有监督学习已经准备好的训练数据输出值，强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的（比如走路摔倒）

    强化学习与非监督学习的区别，在非监督学习中既没有输出值也没有奖励值的，只有数据特征，而强化学习有奖励值（为负是为惩罚），此外非监督学习与监督学习一样，数据之间也都是独立的，没有强化学习这样的前后依赖关系

    监督学习和非监督学习的主要区别：是否有标签

Thinking2 什么是策略网络，价值网络，有何区别

    策略网络就是，对于给定的输入，通过学习给出一个确定输出的网络：状态-行动的配对的列表

    价值网络：通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络。奖励更多的状态，会在数值网络中的数值Value更大。这里的奖励是奖励期望值，我们会从状态集合中选择最优的。.
    
    策略网络的输出，是一个落子的概率分布。价值网络的输出，一个可能获胜的数值，即“价值”，这个价值训练是一种回归(regression)，即调整网络的权重来逼近每一种棋局真实的输赢预测

Thinking3 请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的
    
    蒙特卡洛树搜索，结合了随机模拟的一般性和树搜索的准确性，它采用的各种方法都是为了有效地减少搜索空间。在MCTS的每一个回合，起始内容是一个半展开的搜索树，目标是原先的半展开+再多展开一个/一层节点的搜索树，MCTS的作用是通过模拟来进行预测输出结果，理论上可以用于以{state,action}为定义的任何领域。

    主要步骤：
        选择，从根节点开始，按一定策略，搜索到叶子节点
        扩展，对叶子节点扩展一个或多个合法的子节点
        模拟，对子节点采用随机的方式（这也是为什么称之为蒙特卡洛的原因）模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数.
        回传，根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点

    Step1，选择Select，从根节点往下走，每次都选一个“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续着法的节点，比如 3/3 节点。
    Step2，扩展Expansion，给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”
    Step3，模拟Simluation，用快速走子策略（Rollout policy）走到底，得到一个胜负结果。
    Step4，回传Backup，把模拟的结果加到它的所有父节点上，假设模拟的结果是 0/1，就把0/1加到所有父节点上。

Thinking4： 假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑

    搜索场景：
    在抖音视频领域中，用户浏览观看行为可以看成Markov过程，对马尔科夫决策过程进行建模，实现基于强化学习的排序决策决策模型，可以使得搜索更加智能化

    推荐场景：
    使用强化学习与自适应在线学习，通过持续机器学习和模型优化建立决策引擎，对海量用户行为以及视频特征进行实时分析，帮用户快速发现喜欢的视频

    智能客服：
    将抖音智能客服机器人作为Agent，Agent的决策不是单一节点的直接收益来确定的，而是一个较为长期的人际交互的过程。把消费者与平台的互动看做一个马尔科夫决策过程，使用强化学习，可以建立一个消费者与系统互动的反馈系统。系统的决策是建立在最大化过程收益的基础上 => 达到一个系统与用户的动态平台

    
    广告系统：
    通过强化学习，可以在抖音中投放广告，对于每一位访问用户，根据他们当前状态来决定给他们展示的广告，引导他们的状态向我们希望的方向上转移。

    关键在于构建智能体与环境的反馈机制。


Thinking5：在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路
     
     强化学习中自动驾驶包含了感知、决策和控制三个方面。
     感知指的是如何通过摄像头和其他传感器的输入解析出周围环境的信息，例如有哪些障碍物、障碍物的速度和距离、道路的宽度和曲率等。而感知模块不可能做到完全可靠。Tesla 的无人驾驶事故就是在强光的环境中感知模块失效导致的。强化学习可以做到，即使在某些模块失效的情况下也能做出稳妥的行为。强化学习可以比较容易地学习到一系列的行为。自动驾驶中需要执行一系列正确的行为才能成功的驾驶。如果只有标注数据，学习到的模型每个时刻偏移了一点，到最后可能会偏移非常多，产生毁灭性的后果。强化学习能够学会自动修正偏移。
    
    自动驾驶的决策是指给定感知模块解析出的环境信息如何控制汽车的行为达到驾驶的目标。例如，汽车加速、减速、左转、右转、换道、超车都是决策模块的输出。决策模块不仅需要考虑到汽车的安全性和舒适性，保证尽快到达目标地点，还需要在旁边的车辆恶意的情况下保证乘客的安全。因此，决策模块一方面需要对行车的计划进行长期规划，另一方面需要对周围车辆和行人的行为进行预测。而且，无人驾驶中的决策模块对安全性和可靠性有严格的要求。现有的无人驾驶的决策模块一般是根据规则构建的。虽然基于规则的构建可以应付大部分的驾驶情况，对于驾驶中可能出现的各种各样的突发情况，基于规则的决策系统不可能枚举到所有突发情况。我们需要一种自适应的系统来应对驾驶环境中出现的各种突发情况。


    从零开始通过试错法来学会自动驾驶。无需密集 3D 地图，无需手写规则。





